{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer \n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "import os\n",
    "\n",
    "# txt = extract_text('May 26, 2020.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 25, 2018.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_txt_dict = {}\n",
    "for fl_name in os.listdir('duterte_pdf'):\n",
    "    try:\n",
    "#         pdf_txt_lt.append(extract_text(f'duterte_pdf\\\\{fl_name}'))\n",
    "        pdf_txt_dict[fl_name] = extract_text(f'duterte_pdf\\\\{fl_name}')\n",
    "    except:\n",
    "        print(fl_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">I found that in most PDF it follows a particular structure. The main body of the speech is right after a particular pattern which is the Location and Date of the speech  enclosed by brackets like [Malago Clubhouse, Malacañang Park, Manila | 26 May 2020] then followed by the transcript. So I used regular expression to find the first occurence of this pattern and extract everything below it.\n",
    "\n",
    ">In my obervation there are 2 types of transcript. The first one is when it only contains President Duterte's dialogue second is when there are dialogue from other people.  So I have extract only President Duterte's dialogue. For this type of transcript I have to convert the PDF file to word document for me to determine which are President Duterte's dialogue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Some PDF has the pattern but we can't detect it for some fucking reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we're going to trimmed the top portion which the title, date, and locaiton of the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('\\\\[(.*?)\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June 22, 2019.pdf\n",
      "May 21, 2019.pdf\n",
      "September 4, 2018.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_txt_dict_trimmed = {}\n",
    "for key in pdf_txt_dict.keys():\n",
    "    try:\n",
    "        span = re.search(regex, pdf_txt_dict[key].replace('\\n', '')).span()\n",
    "        pdf_txt_dict_trimmed[key] = pdf_txt_dict[key].replace('\\n', '')[span[1]:]\n",
    "    except:\n",
    "        pdf_txt_dict_trimmed[key] = pdf_txt_dict[key].replace('\\n', '')\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Kindly sit down as I deliver a short speech. I’ve been told to --- masyado naman takot itong corona na ito. They are discouraging long meetings and large congregation. Naniwala pala kayo. Sus. You know if I heard correctly, the health minister of Japan and Abe tested positive for... Interior and Local Government Secretary Eduardo Año and other members of the Cabinet; Senator Bong Go; our newly promoted officers of the Armed Forces of the Philippines and Philippine National Police; other distinguished guests; ladies and gentlemen It is an honor to stand here today as I speak before the most valiant men and women in uniform in our country. First, I congratulate all of you for adding another accolade in your achievements as officials of the Armed Forces of the Philippines and the Philippine National Police. With the many challenges that the world is facing today, we need soldiers like you who are always ready to fight for our motherland’s sovereignty, defend the honor of our country, and secure the liberties of our people that we enjoy. May this significant milestone in your respective careers serve [not just] as a celebration of your personal and professional achievements, but also [as] a reminder of your sworn duty to serve this country and its people. As [you take on a] more challenging role in public service, I am confident that you will remain dedicated and committed in the performance of your respective duties. Let me assure you that this administration stands behind you and will fully support and provide you with the tools and resources that you and your men will need to be able to fulfill your mandate. Indeed, fighting the ills that plague our country is a daunting task that we must continue to do if we are able to realize our shared vision of a safer and stronger Philippines for every Filipino. \\x0cWith your patriotism and bravery --- may I repeat --- with your patriotism and your bravery, I am optimistic that we will be able to achieve our goals and truly serve our people. In solidarity, let us take further strides towards securing a more inclusive and progressive tomorrow for the Republic of the Philippines. Once again, congratulations at mabuhay kayong lahat! [applause] --- END --- Source: PCOO-PND (Presidential News Desk)  \\x0c'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_txt_dict_trimmed ['March 11, 2020.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">May 21, 2019 and September 2018 does not have this pattern so we're going to process this manually. As for June 22, 2019 it is not actually a speech but a press release between Philippines and Indonesia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROBLEM: It doesn't detect it because it contains new line character \n",
    "# 1. SOLUTION: remove breakline character before stripping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PROBLEM: Two speech does not have the pattern\n",
    "\n",
    "# 2. SOLUTION: Process this two individually\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Some PDFs contains dialogue of another person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Unlike an html where we can identify if there is a conversation by searching for strong tag. In PDF there is no such thing, so for us to know if there is a conversation is if it contains one of these strings 'PRESIDENT RODRIGO DUTERTE:', 'PRESIDENT DUTERTE:', 'PRESIDENT RODRIGO ROA DUTERTE:' after trimming it. Then we're probably going to convert these PDF to .docx for us to extract only President Duterte's dialogue.\n",
    "\n",
    ">So we're going to traverse through transcirpts and identify which contains dialogue of another person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_names= ['PRESIDENT RODRIGO DUTERTE:', 'PRESIDENT DUTERTE:', 'PRESIDENT RODRIGO ROA DUTERTE:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['April 13, 2018.pdf', 'April 13, 2020.pdf', 'April 29, 2018.pdf', 'April 8, 2020.pdf', 'April 9, 2018.pdf', 'August 20, 2019.pdf', 'August 30, 2019.pdf', 'December 10, 2019.pdf', 'December 26, 2018.pdf', 'December 4, 2019.pdf', 'December 5, 2019.pdf', 'July 28, 2019.pdf', 'June 2, 2018.pdf', 'June 8, 2019.pdf', 'March 7, 2019.pdf', 'May 21, 2019.pdf', 'May 28, 2020.pdf', 'November 1, 2018.pdf', 'November 15, 2018.pdf', 'November 20, 2018.pdf', 'November 25, 2019.pdf', 'November 6, 2018.pdf', 'October 1, 2019.pdf', 'October 18, 2019.pdf', 'October 2, 2019.pdf', 'October 3, 2019.pdf', 'October 31, 2019.pdf', 'October 6, 2019.pdf', 'September 11, 2018.pdf', 'September 13, 2018.pdf', 'September 16, 2018.pdf', 'September 17, 2018.pdf', 'September 18, 2018.pdf', 'September 2, 2018.pdf']\n"
     ]
    }
   ],
   "source": [
    "lt_docx = []\n",
    "for key in pdf_txt_dict_trimmed.keys():\n",
    "    if any(name in pdf_txt_dict_trimmed[key] for name in lt):\n",
    "        lt_docx.append(key)     \n",
    "print(lt_docx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">So these are the pdfs that contains dialogue of multiple people and not only President Duterte. I converted these manually to docx file and stored them to separate directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PROBLEM: How to identify these PDFs\n",
    "# 3. SOLUTION: Check document contains full name of duterte in capslock followed by ':' (if only a few process individually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW WE'RE GOING TO EXTRACT DIALOGUE OF DUTERTE FROM THOSE DOCXS FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "transcript_docx = {}\n",
    "\n",
    "#traverse through different pdf\n",
    "for fl in lt_docx:\n",
    "    \n",
    "    doc = docx.Document(f'duterte_docx\\\\{fl[:-4]}.docx')\n",
    "    encountered = False\n",
    "    isDuterte = True\n",
    "    trans = []\n",
    "    \n",
    "    #tranverse through the context of transcript\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if encountered:\n",
    "            for run in paragraph.runs:\n",
    "                # if the text is bold we ask if it is a name of duterte or not\n",
    "                if run.bold:\n",
    "                    if any(nm in paragraph.text for nm in lt_names):\n",
    "                        isDuterte = True\n",
    "                    else:\n",
    "                        isDuterte = False\n",
    "            \n",
    "            #we'll only include President Duterte's dialogue in the list\n",
    "            if isDuterte:\n",
    "                if paragraph.text:\n",
    "                    trans.append(paragraph.text)\n",
    "\n",
    "\n",
    "        #if we encounter the pattern except \n",
    "        if re.search(regex, paragraph.text):\n",
    "            encountered = True\n",
    "            \n",
    "    \n",
    "    single_str = ' '.join(trans)\n",
    "    \n",
    "    for nm in lt_names:\n",
    "        single_str = single_str.replace(nm, ' ')\n",
    "        \n",
    "    transcript_docx[fl] = single_str "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PROBLEM 2:How to extract only duterte's part\n",
    "# 3. SOLUTION 2: converte to docx and check if in strong if yes check if duterte if yes extract everything until encoutering strong again then repeat step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll delete May 21, 2019 and September 4, 2018 in both dictionaries because they don't contain the pattern, so we'll process it to manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pdf_txt_dict_trimmed['May 21, 2019.pdf']\n",
    "del transcript_docx['May 21, 2019.pdf']\n",
    "del pdf_txt_dict_trimmed['September 4, 2018.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We'll merge these two dictionary and store them to all_transcript_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_from_pdf = pdf_txt_dict_trimmed\n",
    "transcript_from_pdf.update(pdf_txt_dict_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['April 10, 2018.pdf',\n",
       " 'April 11, 2019.pdf',\n",
       " 'April 12, 2018.pdf',\n",
       " 'April 13, 2018.pdf',\n",
       " 'April 13, 2019.pdf',\n",
       " 'April 13, 2020.pdf',\n",
       " 'April 15, 2018.pdf',\n",
       " 'April 16, 2019.pdf',\n",
       " 'April 16, 2020.pdf',\n",
       " 'April 17, 2018.pdf']"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transcript_from_pdf.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let's remove .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcript_from_pdf = {}\n",
    "for key, val in zip(transcript_from_pdf.keys(), transcript_from_pdf.values()):\n",
    "    all_transcript_from_pdf[key[:-4]] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['April 10, 2018',\n",
       " 'April 11, 2019',\n",
       " 'April 12, 2018',\n",
       " 'April 13, 2018',\n",
       " 'April 13, 2019',\n",
       " 'April 13, 2020',\n",
       " 'April 15, 2018',\n",
       " 'April 16, 2019',\n",
       " 'April 16, 2020',\n",
       " 'April 17, 2018']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_transcript_from_pdf.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we'll add the May 21, 2019 and September 4,2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('duterte_pdf\\\\May 21, 2019.txt', 'r') as fl:\n",
    "    my_21_19 = fl.read()\n",
    "\n",
    "with open('duterte_pdf\\\\September 4, 2018.txt', 'r') as fl:\n",
    "    sp_04_18 = fl.read()\n",
    "\n",
    "all_transcript_from_pdf['May 21, 2019'] = my_21_19.replace('\\n', ' ')\n",
    "all_transcript_from_pdf['September 4, 2018'] = sp_04_18.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your Excellency, I was informed that youâ€™re already here. Pardon for about two minutes late. I was just signing some papers. But I am very happy of your presence today and congratulations to your new deployment. I understand this is the first ambassadorial assignment that you have for the Republic of Thailand. Iâ€™m sure that you would learn so many things along the way. You wonâ€™t have a hard time. We are similar in all things including our desire to have a peaceful southeast Pacific region. Welcome. I accept your lettre de crÃ©ance and just a few minutes may I invite you to a room so we can have a little tÃªte-Ã\\xa0-tÃªte and before that Iâ€™d like to have a picture with you. Iâ€™d learned that a very handsome guy from Thailand visited my country. I accept your assignment. Can we have a picture taking?'"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transcript_from_pdf['May 21, 2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well I would like to thank Israel, the government and the people for inviting us here to see for ourselves in visuals what we have read time and again in our history books. We know the brutal and cruel journey of the Israelites. From one scattered generation finally finding their place under the sun. We are happy that â€” witnesses to this event. And I said my daughter is... I wouldnâ€™t say that now. But she is a descendant of a Jew in the Philippines, probably came there to seek the sanctuary. There was a time when we welcomed Jews in the Philippines to provide them a country of refuge. I cannot speak for my daughter how she feels now but she is one of those generations that came after. Me, I realized that war is insanity. And what happened here, in Europe especially under the Nazi. I could not imagine of a country obey an insane leader. And I could not ever fathom the spectacle of a human being going into a killing spree, murdering old men, women, men, children, mother. I hope that this will not happen again. But we are a world whose â€” now... We have learned so much along the years during the two wars. There is always a lesson to be learned and that despots and leaders who show insanity should be â€” well they should be disposed of at the first instance. I would like to say that we are one in saying that it will not happen again and my country will be the first to voice such I said a massacre of a race just because of hate. Never again.â€©â€© May the world learn the lessons of this horrific and benighted period of human history.â€©â€© May the hearts of peoples around the world remain ever open. And may the minds of all men and women learn to work together towards providing a safe haven for all who are being persecuted.â€©â€©'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transcript_from_pdf['September 4, 2018']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Duterte_Speech.csv', index_col ='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pdf_ser = pd.Series(all_transcript_from_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "April 10, 2018            His Excellency  President Xi Jinping and ...\n",
       "April 11, 2019          Maayong gabi-i.   Te may ara man ko nga sal...\n",
       "April 12, 2018        Salamat po. Thank you for your courtesy. I’d ...\n",
       "April 13, 2018        Kindly sit down. Thank you. Let me just annou...\n",
       "April 13, 2019            Intawon pud ang akong sundalo nga securit...\n",
       "                                           ...                        \n",
       "September 7, 2019       Daghang salamat sa inyong tanan. Panglingko...\n",
       "September 8, 2018         Thank you. Maupo… [crowd: Duterte! Dutert...\n",
       "September 9, 2019         Your Excellency, I warmly welcome you and...\n",
       "May 21, 2019         Your Excellency, I was informed that youâ€™re ...\n",
       "September 4, 2018    Well I would like to thank Israel, the governm...\n",
       "Length: 297, dtype: object"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_pdf_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.index.name = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('Duterte_Speech2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now that we have every transcript let's merge them into our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.merge(df.reset_index(), temp_sr.to_frame().T.reset_index(), how='outer').set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in zip(all_transcript_from_pdf.keys(), all_transcript_from_pdf.values()):\n",
    "    df_2.loc[key]['transcript'] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">So far we have one transcript that does not work which is the speech on April 25, 2018(the pdf url does not work), and we also have a transcript that does belong to duterte which is on June 22, 2019. So we'll remove this from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final= df_2.drop(index=['June 22, 2019', 'April 25, 2018'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 5)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS THE TRANSCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     So ako pati si Bong during my mayorship days... So at that moment was si Bong ang… Sabi niya, “Ako na lang muna.” Sabi niya, “Mayor, kung wala kang ibang makuha kasi…” Right there and then nasabi ko somebody should call the funeral parlor. So siya ‘yun lahat until he became… Siya ‘yung kasama ko.   So in and out kami sa --- in and out kami sa gubat. And as a matter of fact, Colonel Lorenzana was there. We’re friends. Kumakain ako sa kampo niya. But at times, galing ako sa bukid nakikiusap ako sa mga NPA.   Ako naman hindi ako nag --- I’m not trying to really pull my own chair but for all ‘yung naka --- nung mga sundalo pati pulis na nabihag ng --- except si ‘yung general --- lahat ‘yan ako ang kumukuha pati si Bong. Kaya bilib ako. Saludo ako kung ginawa ninyo.   I’m not paying you. I know that you are not mercenaries. You do not do it for money. You do it out of passion for the love of country because we are all Filipinos and we know where to place ourselves when the time comes kung na…   Kayo nandiyan na ngayon adulthood, kayong mga sundalo, piloto. Ako naman, by stroke of fate lang, I became President. [applause]     --- END ---  \\x0c'"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.iloc[0].transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_transcript(single_str):\n",
    "    lt_names= ['PRESIDENT RODRIGO DUTERTE:', 'PRESIDENT DUTERTE:', 'PRESIDENT RODRIGO ROA DUTERTE:']\n",
    "    str_cleaned = single_str\n",
    "    for nm in lt_names:\n",
    "        str_cleaned = str_cleaned.replace(nm, '')\n",
    "    str_cleaned = str_cleaned.replace('\\xa0', '')\n",
    "    str_cleaned = str_cleaned.replace('\\x0c', '')\n",
    "    str_cleaned = str_cleaned.replace('\\\\', '')\n",
    "    str_cleaned = str_cleaned.replace('—', '')\n",
    "    str_cleaned = str_cleaned.replace('-', '')\n",
    "    str_cleaned = str_cleaned.replace('...', '')\n",
    "    str_cleaned = str_cleaned.replace('…', '')\n",
    "    str_cleaned = str_cleaned.replace('END', '')\n",
    "    str_cleaned = re.sub(r\"\\[(.*?)\\]\", '', str_cleaned) #remove all that is contained within brackets\n",
    "    str_cleaned = re.sub(r\"\\((.*?)\\)\", '', str_cleaned) #remove all that is contained within parentheses\n",
    "    str_cleaned = re.sub(r'\\“(.+?)\\”', '', str_cleaned) #remove all that is in quatation as he probably quoting someone\n",
    "    str_cleaned = str_cleaned.replace('‘', '')\n",
    "    str_cleaned = [wrd.lower() for wrd in str_cleaned.split(' ') if wrd != '']\n",
    "    str_cleaned = ' '.join(str_cleaned)\n",
    "\n",
    "    return str_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "all_stop_words = []\n",
    "with open('Filipino_Stopwords.txt', 'r') as fl:\n",
    "    for n in fl:\n",
    "        all_stop_words.append(n.strip())\n",
    "\n",
    "all_stop_words.extend(ENGLISH_STOP_WORDS)\n",
    "\n",
    "def create_word_cloud(ser, stop_words=all_stop_words, ax=plt):\n",
    "    full_txt = ''\n",
    "    for txt in ser:\n",
    "        full_txt += txt\n",
    "    \n",
    "    wc = WordCloud(stopwords=stop_words, background_color='white', colormap='Dark2', random_state=42)\n",
    "    cleaned_temp = [word for word in full_txt.split(' ') if word not in all_stop_words]\n",
    "    txt = ' '.join(cleaned_temp)\n",
    "    wc.generate(txt)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_venv_2",
   "language": "python",
   "name": "data_science_venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
